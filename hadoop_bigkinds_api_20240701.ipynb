{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().strftime(\"%Y%m%d\")\n",
    "base_url = \"https://news.naver.com/main/list.naver?mode=LSD&mid=shm&sid1=101&date=\" + today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_links(page):\n",
    "    url = base_url + \"&page=\" + str(page)\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.text, \"html.parser\")\n",
    "    \n",
    "    links = []\n",
    "    for a in soup.select(\"ul.type06_headline li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    for a in soup.select(\"ul.type06 li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_news_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.text, \"html.parser\")\n",
    "    \n",
    "    title_tag = soup.select_one(\"h2.media_end_head_headline\")\n",
    "    content_tag = soup.find('article',{'id':'dic_area'})\n",
    "    \n",
    "    if title_tag and content_tag:\n",
    "        title = title_tag.get_text().strip()\n",
    "        content = content_tag.get_text().strip()\n",
    "        return title, content\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 시작\n",
    "news_links = []\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    links = get_news_links(page)\n",
    "    if not links or any(link in news_links for link in links):\n",
    "        break\n",
    "    news_links.extend(links)\n",
    "    page += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 기사 내용 크롤링\n",
    "news_contents = []\n",
    "for link in news_links[:10]:    # 숫자 변경\n",
    "    try:\n",
    "        title, content = get_news_content(link)\n",
    "        news_contents.append((title, content))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get content from {link}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer # pip install sentence_transformers\n",
    "from bareunpy import Tagger # pip install bareunpy\n",
    "\n",
    "tagger = Tagger('koba-Q2CYNCI-XZ7E7PI-X6YRKPY-K4Z2KMY')\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_ext(text):\n",
    "\n",
    "    tokenized_doc = tagger.pos(text)\n",
    "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'NNG' or word[1] == 'NNP'])\n",
    "\n",
    "    n_gram_range = (1,1)\n",
    "\n",
    "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    doc_embedding = model.encode([text])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    return mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    # print(keywords_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Title                       Keywords\n",
      "0            LG에너지솔루션, 호주 리튬 광산 대규모 투자…펀더멘탈 강화        [리튬, 협약, 수산화리튬, 전기, 계약]\n",
      "1            남양유업, 경영진 교체 후 대리점 첫 만남...“협력 지속”        [기업, 서울, 회의실, 남양유업, 사장]\n",
      "2          [속보]최수연 네이버 대표 \"단기적으로 매각 결정하지 않을 것\"          [회의, 국회, 참석, 대표, 네이버]\n",
      "3  \"개인맞춤형 헬스케어 사업 확대\"..광동제약, 체외진단기기사 '프리시젼' 인수      [주식, 기업, 인수, 매매, 프리시젼바이오]\n",
      "4                 코레일, 카이스트와 '철도표준 모빌리티 학과' 설립      [직원, 한국철도공사, 총장, 김동규, 사장]\n",
      "5          인질잡힌 환율에 운신폭 제한…\"재정·통화 정책조합 묘수 찾아야\"  [금리, 인플레이션, 현대경제연구원, 위기, 소득세]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store titles and nouns\n",
    "df = pd.DataFrame(columns=[\"Title\", \"Keywords\"])\n",
    "\n",
    "# Text analysis using Mecab and store in DataFrame\n",
    "for i, (title, content) in enumerate(set(news_contents), start=1):\n",
    "    if content:\n",
    "        keywords = keyword_ext(content)\n",
    "        new_row = pd.DataFrame({\"Title\": [title], \"Keywords\": [keywords]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)        \n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하둡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to HDFS at /test/2024-07-01.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pyarrow import fs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pyarrow.csv as pc\n",
    "# Get the current date and format it as a string\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define the file path in HDFS\n",
    "\n",
    "#hdfs dfs -mkdir test\n",
    "file_path = f\"/test{today}.csv\"\n",
    "classpath = subprocess.Popen([\"/home/ksk/hadoop/bin/hdfs\", \"classpath\", \"--glob\"], stdout=subprocess.PIPE).communicate()[0]\n",
    "os.environ[\"CLASSPATH\"] = classpath.decode(\"utf-8\")\n",
    "hdfs = fs.HadoopFileSystem(host='192.168.0.206', port=8020, user='ksk')\n",
    "\n",
    "# Pandas DataFrame을 PyArrow의 Table 객체로 변환\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Write the table to HDFS as a Parquet file\n",
    "with hdfs.open_output_stream(file_path) as stream:\n",
    "    # pc.write_table(table, stream)\n",
    "    pc.write_csv(table,stream)\n",
    "print(f\"DataFrame saved to HDFS at {file_path}\")\n",
    "    \n",
    "# # PyArrow를 사용하여 Parquet 포맷으로 데이터 저장\n",
    "# pq.write_table(table, file_path, filesystem=hdfs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdfs dfs -mkdir test\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"hdfs://192.168.0.206:8020/test/2024-06-28.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8734"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f026282f7f0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDF, 1378.8원/1379.2원…4.65원 상승</td>\n",
       "      <td>원·달러 1개월물 스와프 포인트 -2.35원[이데일리 하상렬 기자] 간밤 뉴욕 차액...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NH투자증권, 부동산PF 나홀로 선방…2Q 영업익 2230억대 기대</td>\n",
       "      <td>서울 여의도 NH투자증권 사옥 전경  /사진 제공=NH투자증권금융권의 뇌관으로 지목...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5월 서울 아파트 거래량 5천여 건, 3년 만에 최대</td>\n",
       "      <td>(서울=연합뉴스) 김도훈 기자 = 5월 서울 아파트 거래량이 5천여 건에 달하는 등...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>신협중앙회, 1조원 규모 부실채권 정리 추진</td>\n",
       "      <td>부동산 및 건설업 관련 충당금 추가 적립\\n\\n\\n\\n신협중앙회는 1일 총 1조원 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'기후동행카드' 본사업 시작…\"단기권 출시·할인 혜택 확대\"</td>\n",
       "      <td>6개월 간 시범 운영 기간 시민 의견 반영단기권 구매자도 할인 혜택 받을 수 있어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>부산창경, 30억 원 한국모태펀드 투자금 결성 완료</td>\n",
       "      <td>부산창조경제혁신센터(이하 부산창경)가 지역 스타트업의 성장을 위한 30억 원 규모의...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>컨테이너 가득 쌓인 부산항</td>\n",
       "      <td>[부산=뉴시스] 하경민 기자 = 산업통상자원부는 지난달 수출이 전년보다 5.1% 증...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>'한전 납품' 상장사 아이앤씨, 하청업체 갑질 '덜미'</td>\n",
       "      <td>공정위, 하도급법 위반으로 시정명령 벌점 누적되면 공공입찰 참가 제한\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>\"밸류업 인센티브 구체화…'주주환원' 여력 지주사 주목\"</td>\n",
       "      <td>SK증권 보고서[이데일리 이용성 기자] 밸류업 프로그램에 대한 인센티브가 구체화하고...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>애큐온저축銀, 최대 '연 3.95%' 정기예금 출시</td>\n",
       "      <td>애큐온저축은행은 오늘(1일) 최대 연 3.95%의 금리를 제공하는 정기예금 상품 '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6998 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  \\\n",
       "0             NDF, 1378.8원/1379.2원…4.65원 상승   \n",
       "1     NH투자증권, 부동산PF 나홀로 선방…2Q 영업익 2230억대 기대   \n",
       "2             5월 서울 아파트 거래량 5천여 건, 3년 만에 최대   \n",
       "3                  신협중앙회, 1조원 규모 부실채권 정리 추진   \n",
       "4         '기후동행카드' 본사업 시작…\"단기권 출시·할인 혜택 확대\"   \n",
       "...                                     ...   \n",
       "6993           부산창경, 30억 원 한국모태펀드 투자금 결성 완료   \n",
       "6994                         컨테이너 가득 쌓인 부산항   \n",
       "6995         '한전 납품' 상장사 아이앤씨, 하청업체 갑질 '덜미'   \n",
       "6996        \"밸류업 인센티브 구체화…'주주환원' 여력 지주사 주목\"   \n",
       "6997           애큐온저축銀, 최대 '연 3.95%' 정기예금 출시   \n",
       "\n",
       "                                                content  \n",
       "0     원·달러 1개월물 스와프 포인트 -2.35원[이데일리 하상렬 기자] 간밤 뉴욕 차액...  \n",
       "1     서울 여의도 NH투자증권 사옥 전경  /사진 제공=NH투자증권금융권의 뇌관으로 지목...  \n",
       "2     (서울=연합뉴스) 김도훈 기자 = 5월 서울 아파트 거래량이 5천여 건에 달하는 등...  \n",
       "3     부동산 및 건설업 관련 충당금 추가 적립\\n\\n\\n\\n신협중앙회는 1일 총 1조원 ...  \n",
       "4     6개월 간 시범 운영 기간 시민 의견 반영단기권 구매자도 할인 혜택 받을 수 있어 ...  \n",
       "...                                                 ...  \n",
       "6993  부산창조경제혁신센터(이하 부산창경)가 지역 스타트업의 성장을 위한 30억 원 규모의...  \n",
       "6994  [부산=뉴시스] 하경민 기자 = 산업통상자원부는 지난달 수출이 전년보다 5.1% 증...  \n",
       "6995  공정위, 하도급법 위반으로 시정명령 벌점 누적되면 공공입찰 참가 제한\\n\\n\\n\\n...  \n",
       "6996  SK증권 보고서[이데일리 이용성 기자] 밸류업 프로그램에 대한 인센티브가 구체화하고...  \n",
       "6997  애큐온저축은행은 오늘(1일) 최대 연 3.95%의 금리를 제공하는 정기예금 상품 '...  \n",
       "\n",
       "[6998 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "첫 번째 방법은 HDFS의 출력 스트림을 명시적으로 열고 닫으며, 스트림을 직접 다룰 수 있는 유연성을 제공합니다. 이 방법은 대용량 데이터를 점진적으로 쓰거나 스트림 제어가 필요한 상황에서 유리합니다.\n",
    "두 번째 방법은 PyArrow의 고수준 API를 사용하여 간단하고 직관적으로 데이터를 HDFS에 저장합니다. 코드가 간결하고 파일 시스템을 추상화하기 때문에 다양한 파일 시스템에도 쉽게 적용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
