{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().strftime(\"%Y%m%d\")\n",
    "base_url = \"https://news.naver.com/main/list.naver?mode=LSD&mid=shm&sid1=101&date=\" + today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_links(page):\n",
    "    url = base_url + \"&page=\" + str(page)\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.text, \"html.parser\")\n",
    "    \n",
    "    links = []\n",
    "    for a in soup.select(\"ul.type06_headline li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    for a in soup.select(\"ul.type06 li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_news_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.text, \"html.parser\")\n",
    "    \n",
    "    title_tag = soup.select_one(\"h2.media_end_head_headline\")\n",
    "    content_tag = soup.find('article',{'id':'dic_area'})\n",
    "    \n",
    "    if title_tag and content_tag:\n",
    "        title = title_tag.get_text().strip()\n",
    "        content = content_tag.get_text().strip()\n",
    "        return title, content\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 시작\n",
    "news_links = []\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    links = get_news_links(page)\n",
    "    if not links or any(link in news_links for link in links):\n",
    "        break\n",
    "    news_links.extend(links)\n",
    "    page += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 기사 내용 크롤링\n",
    "news_contents = []\n",
    "for link in news_links[:10]:    # 숫자 변경\n",
    "    try:\n",
    "        title, content = get_news_content(link)\n",
    "        news_contents.append((title, content))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get content from {link}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer # pip install sentence_transformers\n",
    "from bareunpy import Tagger # pip install bareunpy\n",
    "\n",
    "tagger = Tagger('koba-Q2CYNCI-XZ7E7PI-X6YRKPY-K4Z2KMY')\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_ext(text):\n",
    "\n",
    "    tokenized_doc = tagger.pos(text)\n",
    "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'NNG' or word[1] == 'NNP'])\n",
    "\n",
    "    n_gram_range = (1,1)\n",
    "\n",
    "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    doc_embedding = model.encode([text])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    return mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    # print(keywords_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Title                       Keywords\n",
      "0            LG에너지솔루션, 호주 리튬 광산 대규모 투자…펀더멘탈 강화        [리튬, 협약, 수산화리튬, 전기, 계약]\n",
      "1            남양유업, 경영진 교체 후 대리점 첫 만남...“협력 지속”        [기업, 서울, 회의실, 남양유업, 사장]\n",
      "2          [속보]최수연 네이버 대표 \"단기적으로 매각 결정하지 않을 것\"          [회의, 국회, 참석, 대표, 네이버]\n",
      "3  \"개인맞춤형 헬스케어 사업 확대\"..광동제약, 체외진단기기사 '프리시젼' 인수      [주식, 기업, 인수, 매매, 프리시젼바이오]\n",
      "4                 코레일, 카이스트와 '철도표준 모빌리티 학과' 설립      [직원, 한국철도공사, 총장, 김동규, 사장]\n",
      "5          인질잡힌 환율에 운신폭 제한…\"재정·통화 정책조합 묘수 찾아야\"  [금리, 인플레이션, 현대경제연구원, 위기, 소득세]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store titles and nouns\n",
    "df = pd.DataFrame(columns=[\"Title\", \"Keywords\"])\n",
    "\n",
    "# Text analysis using Mecab and store in DataFrame\n",
    "for i, (title, content) in enumerate(set(news_contents), start=1):\n",
    "    if content:\n",
    "        keywords = keyword_ext(content)\n",
    "        new_row = pd.DataFrame({\"Title\": [title], \"Keywords\": [keywords]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)        \n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하둡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pyarrow import fs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdfs dfs -mkdir test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date and format it as a string\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define the file path in HDFS\n",
    "\n",
    "#hdfs dfs -mkdir test\n",
    "file_path = f\"/test/{current_date}.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 09:32:36,092 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "classpath = subprocess.Popen([\"/home/ksk/hadoop/bin/hdfs\", \"classpath\", \"--glob\"], stdout=subprocess.PIPE).communicate()[0]\n",
    "os.environ[\"CLASSPATH\"] = classpath.decode(\"utf-8\")\n",
    "hdfs = fs.HadoopFileSystem(host='192.168.0.206', port=8020, user='ksk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to HDFS at /test/2024-07-02.parquet\n"
     ]
    }
   ],
   "source": [
    "# Pandas DataFrame을 PyArrow의 Table 객체로 변환\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Write the table to HDFS as a Parquet file\n",
    "with hdfs.open_output_stream(file_path) as stream:\n",
    "    pq.write_table(table, stream)\n",
    "\n",
    "print(f\"DataFrame saved to HDFS at {file_path}\")\n",
    "    \n",
    "# # PyArrow를 사용하여 Parquet 포맷으로 데이터 저장\n",
    "# pq.write_table(table, file_path, filesystem=hdfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "첫 번째 방법은 HDFS의 출력 스트림을 명시적으로 열고 닫으며, 스트림을 직접 다룰 수 있는 유연성을 제공합니다. 이 방법은 대용량 데이터를 점진적으로 쓰거나 스트림 제어가 필요한 상황에서 유리합니다.\n",
    "두 번째 방법은 PyArrow의 고수준 API를 사용하여 간단하고 직관적으로 데이터를 HDFS에 저장합니다. 코드가 간결하고 파일 시스템을 추상화하기 때문에 다양한 파일 시스템에도 쉽게 적용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
