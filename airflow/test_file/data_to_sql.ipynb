{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, size, explode, monotonically_increasing_id\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CountVectorizer_sklearn\n",
    "from pyspark.ml.feature import CountVectorizer as CountVectorizer_pyspark\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bareunpy import Tagger\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "from pyarrow import fs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "classpath = subprocess.Popen([\"/home/ksk/hadoop/bin/hdfs\", \"classpath\", \"--glob\"], stdout=subprocess.PIPE).communicate()[0]\n",
    "os.environ[\"CLASSPATH\"] = classpath.decode(\"utf-8\")\n",
    "hdfs = fs.HadoopFileSystem(host='192.168.0.206', port=8020, user='ksk')\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName(\"gen\")\\\n",
    "        .setMaster(\"spark://master:7077\")\\\n",
    "        .set(\"spark.executor.instances\", \"3\")\\\n",
    "        .set(\"spark.jars\",\"/opt/spark/jars/mysql-connector-j-9.0.0.jar\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# 모델 초기화\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model_broadcast = spark.sparkContext.broadcast(model)\n",
    "\n",
    "# Tagger를 초기화하는 함수\n",
    "def keyword_ext(texts):\n",
    "    tagger = Tagger('koba-WUGVF6Q-OOVUL2I-QIXBCLQ-JRCUAAY')\n",
    "    model = model_broadcast.value\n",
    "    results = []\n",
    "   \n",
    "    for text in texts:\n",
    "        if text is None or not text.strip():\n",
    "            results.append([])\n",
    "            continue\n",
    "\n",
    "        tokenized_doc = tagger.pos(text)  # phrase 인수로 text를 전달\n",
    "        tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'NNG' or word[1] == 'NNP'])\n",
    "        if not tokenized_nouns.strip():\n",
    "            results.append([])\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            count = CountVectorizer_sklearn(ngram_range=(1, 1)).fit([tokenized_nouns])\n",
    "            candidates = count.fit([tokenized_nouns]).get_feature_names_out()\n",
    "        except ValueError as e:\n",
    "            if str(e) == 'empty vocabulary; perhaps the documents only contain stop words':\n",
    "                results.append([])\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "        if len(candidates) == 0:\n",
    "            results.append([])\n",
    "            continue\n",
    "\n",
    "        doc_embedding = model.encode([text])\n",
    "        candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "        keywords = mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)\n",
    "        results.append(keywords)\n",
    "\n",
    "    return results\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "    \"\"\"다양성을 고려한 최대 마진 적중률(MMR)을 계산하여 키워드를 추출합니다.\"\"\"\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    if len(word_doc_similarity) == 0 or len(word_similarity) == 0:\n",
    "        return []\n",
    "\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "        mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "\n",
    "        if mmr.size == 0:\n",
    "            break\n",
    "\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n",
    "\n",
    "\n",
    "# 뉴스 내용을 포함한 CSV 파일 읽기\n",
    "today = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "# news_contents_df = spark.read.csv(f\"hdfs:/test/{today}.csv\", header=True, inferSchema=True, encoding=\"utf-8\")\n",
    "news_contents_df = spark.read.csv(\"hdfs:/test/2024-07-06.csv\", header=True, inferSchema=True, encoding=\"utf-8\")\n",
    "# keyword_ext UDF 적용하여 키워드 추출\n",
    "news_rdd = news_contents_df.select(\"content\").rdd.map(lambda row: row[0])\n",
    "keywords_rdd = news_rdd.mapPartitions(keyword_ext)\n",
    "keywords_df = keywords_rdd.zipWithIndex().toDF([\"Keywords\", \"Index\"])\n",
    "\n",
    "# 원본 데이터프레임과 조인\n",
    "news_contents_df = news_contents_df.withColumn(\"Index\", monotonically_increasing_id())\n",
    "news_contents_df = news_contents_df.join(keywords_df, on=\"Index\").drop(\"Index\")\n",
    "\n",
    "# Keywords가 비어 있는 행은 필터링합니다.\n",
    "news_contents_df = news_contents_df.filter(size(col(\"Keywords\")) > 0)\n",
    "\n",
    "from pyspark.sql.functions import udf, col, explode\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# CountVectorizer를 사용하여 키워드 빈도 계산 준비\n",
    "cv = CountVectorizer_pyspark(inputCol=\"Keywords\", outputCol=\"KeywordCounts\")\n",
    "cv_model = cv.fit(news_contents_df)\n",
    "keyword_counts_df = cv_model.transform(news_contents_df)\n",
    "\n",
    "# 각 키워드의 개수를 추출하는 UDF 정의\n",
    "def extract_keywords(vec, vocab):\n",
    "    indices = vec.indices\n",
    "    return [vocab[i] for i in indices]\n",
    "\n",
    "vocab = cv_model.vocabulary\n",
    "extract_keywords_udf = udf(lambda vec: extract_keywords(vec, vocab), ArrayType(StringType()))\n",
    "\n",
    "# KeywordCounts 열을 사용하여 키워드 추출\n",
    "keywords_exploded_df = keyword_counts_df.withColumn(\"KeywordsList\", extract_keywords_udf(col(\"KeywordCounts\")))\n",
    "\n",
    "# 각 키워드를 개별 행으로 펼침\n",
    "keywords_exploded_df = keywords_exploded_df.withColumn(\"Keyword\", explode(col(\"KeywordsList\")))\n",
    "\n",
    "# 키워드별로 그룹화하고 발생 횟수 계산\n",
    "final_keyword_counts = keywords_exploded_df.groupBy(\"Keyword\").count()\n",
    "\n",
    "sorted_final_keyword_counts = final_keyword_counts.orderBy(col(\"count\").desc())\n",
    "\n",
    "# MySQL 연결 설정\n",
    "jdbc_url = \"jdbc:mysql://43.202.5.70:3306/encore_web\"\n",
    "db_properties = {\n",
    "    \"user\": \"class5\",\n",
    "    \"password\": \"123\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "# 데이터프레임을 MySQL 데이터베이스에 저장\n",
    "sorted_final_keyword_counts.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"2024_07_06\") \\\n",
    "    .option(\"user\", db_properties[\"user\"]) \\\n",
    "    .option(\"password\", db_properties[\"password\"]) \\\n",
    "    .option(\"driver\", db_properties[\"driver\"]) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "# .option(\"dbtable\", f\"{today}\") \\\n",
    "\n",
    "################# pyspark와 bareunAPI를 이용한 전처리 및 sql 저장 과정###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file /test/2024-07-11.csv: For input string: \"i\"\n",
      "Retrying... (1/5)\n",
      "Error reading file /test/2024-07-11.csv: For input string: \"i\"\n",
      "Retrying... (2/5)\n",
      "Data successfully saved to table 2024-07-11.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from urllib import parse\n",
    "import subprocess\n",
    "from hdfs import InsecureClient\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "import pymysql\n",
    "import time\n",
    "\n",
    "mecab = Mecab()\n",
    "# today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "today = '2024-07-11'\n",
    "client_hdfs = InsecureClient(\"http://192.168.0.206:50070\")\n",
    "\n",
    "# HDFS 파일 읽기 함수\n",
    "def read_hdfs_file(client, path, max_retries=5, delay=5):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            with client.read(path) as reader:\n",
    "                return pd.read_csv(reader, encoding='utf-8')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {path}: {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying... ({retries}/{max_retries})\")\n",
    "                time.sleep(delay)\n",
    "    raise Exception(f\"Failed to read file {path} after {max_retries} attempts\")\n",
    "\n",
    "# 뉴스 내용을 포함한 CSV 파일 읽기\n",
    "news_contents = read_hdfs_file(client_hdfs, f'/test/{today}.csv')\n",
    "news_contents = news_contents.dropna()\n",
    "\n",
    "tmp = []\n",
    "for content in news_contents['content']:\n",
    "    word = mecab.nouns(content)\n",
    "    tmp.extend(word)\n",
    "\n",
    "nouns = [t for t in tmp if len(t) > 1]\n",
    "\n",
    "df = pd.DataFrame.from_dict(Counter(nouns), orient='index').reset_index()\n",
    "df.columns = ['Keyword', 'count']\n",
    "df = df.sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "user = 'class5'\n",
    "password = '123'\n",
    "host = '43.202.5.70'\n",
    "port = 3306\n",
    "database = 'encore_web'\n",
    "password = parse.quote_plus(password)\n",
    "\n",
    "# pymysql을 사용하여 MySQL에 연결\n",
    "connection = pymysql.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database=database,\n",
    "    port=port,\n",
    "    charset='utf8mb4',\n",
    "    cursorclass=pymysql.cursors.DictCursor\n",
    ")\n",
    "\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        # 테이블이 없으면 생성\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS `{today}` (\n",
    "            `Keyword` VARCHAR(255) NOT NULL,\n",
    "            `count` INT NOT NULL\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        \n",
    "        # 데이터 삽입\n",
    "        for index, row in df.iterrows():\n",
    "            insert_query = f\"\"\"\n",
    "            INSERT INTO `{today}` (`Keyword`, `count`)\n",
    "            VALUES (%s, %s)\n",
    "            ON DUPLICATE KEY UPDATE `count` = VALUES(`count`)\n",
    "            \"\"\"\n",
    "            cursor.execute(insert_query, (row['Keyword'], row['count']))\n",
    "    \n",
    "    connection.commit()\n",
    "    print(f\"Data successfully saved to table {today}.\")\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n",
    "finally:\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "################# mecab을 이용한 전처리 및 sql 저장 과정###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
