{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().strftime(\"%Y%m%d\")\n",
    "base_url = \"https://news.naver.com/main/list.naver?mode=LSD&mid=shm&sid1=101&date=\" + today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_links(page):\n",
    "    url = base_url + \"&page=\" + str(page)\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.text, \"html.parser\")\n",
    "    \n",
    "    links = []\n",
    "    for a in soup.select(\"ul.type06_headline li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    for a in soup.select(\"ul.type06 li dl dt a\"):\n",
    "        links.append(a[\"href\"])\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_news_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.text, \"html.parser\")\n",
    "    \n",
    "    title_tag = soup.select_one(\"h2.media_end_head_headline\")\n",
    "    content_tag = soup.find('article',{'id':'dic_area'})\n",
    "    \n",
    "    if title_tag and content_tag:\n",
    "        title = title_tag.get_text().strip()\n",
    "        content = content_tag.get_text().strip()\n",
    "        return title, content\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 시작\n",
    "news_links = []\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    links = get_news_links(page)\n",
    "    if not links or any(link in news_links for link in links):\n",
    "        break\n",
    "    news_links.extend(links)\n",
    "    page += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 기사 내용 크롤링\n",
    "news_contents = []\n",
    "for link in news_links[:20]:    # 숫자 변경\n",
    "    try:\n",
    "        title, content = get_news_content(link)\n",
    "        news_contents.append((title, content))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get content from {link}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer # pip install sentence_transformers\n",
    "from bareunpy import Tagger # pip install bareunpy\n",
    "\n",
    "tagger = Tagger('koba-Q2CYNCI-XZ7E7PI-X6YRKPY-K4Z2KMY')\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_ext(text):\n",
    "\n",
    "    tokenized_doc = tagger.pos(text)\n",
    "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'NNG' or word[1] == 'NNP'])\n",
    "\n",
    "    n_gram_range = (1,1)\n",
    "\n",
    "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    doc_embedding = model.encode([text])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    return mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    # print(keywords_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Keywords  Count\n",
      "0          서울    792\n",
      "1          시장    508\n",
      "2        2024    309\n",
      "3          가격    297\n",
      "4          한국    290\n",
      "...       ...    ...\n",
      "5618    매봉산의료      1\n",
      "5619     영주장날      1\n",
      "5620    박셀바이오      1\n",
      "5621      김영한      1\n",
      "5622      독과점      1\n",
      "\n",
      "[5623 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bareunpy import Tagger\n",
    "from datetime import datetime\n",
    "\n",
    "def keyword_ext(text):\n",
    "    \"\"\"텍스트에서 키워드를 추출합니다.\"\"\"\n",
    "    tokenized_doc = tagger.pos(text)\n",
    "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'NNG' or word[1] == 'NNP'])\n",
    "    if not tokenized_nouns.strip():\n",
    "        return []\n",
    "    \n",
    "    count = CountVectorizer(ngram_range=(1, 1)).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    if len(candidates) == 0:\n",
    "        return []\n",
    "\n",
    "    doc_embedding = model.encode([text])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    return mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "    \"\"\"다양성을 고려한 최대 마진 적중률(MMR)을 계산하여 키워드를 추출합니다.\"\"\"\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    if len(word_doc_similarity) == 0 or len(word_similarity) == 0:\n",
    "        return []\n",
    "\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "        mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "\n",
    "        if mmr.size == 0:\n",
    "            break\n",
    "\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n",
    "\n",
    "# 뉴스 내용을 포함한 CSV 파일 읽기\n",
    "today = datetime.today().strftime(\"%Y%m%d\")\n",
    "# news_contents = pd.read_csv(f\"{today}.csv\", encoding='utf-8')\n",
    "\n",
    "news_contents = pd.read_csv(\"20240701.csv\", encoding='utf-8')\n",
    "news_contents = news_contents.dropna()\n",
    "\n",
    "# 제목과 키워드를 저장할 DataFrame 생성\n",
    "df = pd.DataFrame(columns=[\"Title\", \"Keywords\", \"Keyword_Count\"])\n",
    "\n",
    "# 키워드 추출 및 DataFrame에 저장\n",
    "for i, row in news_contents.iterrows():\n",
    "    title = row['Title']\n",
    "    content = row['content']\n",
    "    if content:\n",
    "        keywords = keyword_ext(content)\n",
    "        new_row = pd.DataFrame({\"Title\": [title], \"Keywords\": [keywords], \"Keyword_Count\": [len(keywords)]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# DataFrame 출력\n",
    "# print(df)\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "# df.to_csv(f\"{today}_keywords.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "# 키워드와 그 빈도를 저장할 DataFrame 생성\n",
    "all_keywords = []\n",
    "\n",
    "for keywords in df['Keywords']:\n",
    "    all_keywords.extend(keywords)\n",
    "\n",
    "keyword_counts = pd.Series(all_keywords).value_counts().reset_index()\n",
    "keyword_counts.columns = ['Keywords', 'Count']\n",
    "\n",
    "# 최종 키워드 빈도수 DataFrame 출력\n",
    "print(keyword_counts)\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "# keyword_counts.to_csv(f\"{today}_keyword_counts.csv\", index=False, encoding='utf-8-sig')\n",
    "keyword_counts.to_csv(\"20240701_keyword_counts.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'갈길 바쁜데' 총파업 발목잡은 삼성전자 노조..수주 리스크 우려</td>\n",
       "      <td>지난 5월29일 삼성전자 서초사옥 앞에서 전국삼성전자노동조합 파업 선언 기자회견이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>트립닷컴, 여름 휴가철 슈퍼세일…도쿄 왕복 항공권 5만원</td>\n",
       "      <td>트립닷컴이 본격적인 여름 휴가철을 맞아 인기 여행지를 위한 할인 프로모션을 시작한다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>공정위, 사행적 판매원 확장한 워너비데이터 시정명령…檢 고발</td>\n",
       "      <td>방문판매법 위반행위 제재…영업정지명령\\n\\n\\n\\n공정거래위원회 ⓒ연합뉴스[세종=데...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>경기도, 리스로 고가 수입차 타는 고액 체납자 456명 적발</td>\n",
       "      <td>리스보증금 55억원 압류…가택수색·동산압류·형사고발도 추진(수원=연합뉴스) 김경태 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‘엔비디아 훈풍’ 단일종목 채권혼합 ETF, 순자산 3배 급증</td>\n",
       "      <td>6개월새 순자산 5100억으로 쑥급등株에 채권혼합 안전성 더해퇴직연금서 '대규모 자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>DL이앤씨, 6년 연속 건설업계 최고 신용등급 'AA-'</td>\n",
       "      <td>\"수익성 중심 내실 경영과 리스크 관리로 기업가치 제고\"DL이앤씨가 한국신용평가, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332</th>\n",
       "      <td>빙그레 더위사냥·생귤탱귤 '제로' 나왔다</td>\n",
       "      <td>[사진=빙그레 제공]빙그레가 당류가 빠진 빙과 신제품 2종을 선보인다고 오늘(2일)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5333</th>\n",
       "      <td>‘골든타임을 지키는 다양한 가족’ 배유미 씨, 한미연 영상 공모전 대상</td>\n",
       "      <td>한반도미래인구연구원,‘2024 다양한 가족의 재발견 영상 공모전’ 총 9팀 선정“입...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334</th>\n",
       "      <td>조달청, 올해 1차 품질보증조달물품 지정서 수여…34개사 81개 물품</td>\n",
       "      <td>안전운전 돕는 '교통안전표지'·'정수처리용 산기장치' 등… 납품검사 면제 등 혜택 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5335</th>\n",
       "      <td>맥도날드, 시각 장애인 위한 ‘음성 안내’ 키오스크 도입</td>\n",
       "      <td>지난해 9월 첫 설치…전국 직영 매장 100% 도입 완료\\n\\n\\n\\n한국맥도날드가...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5336 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Title  \\\n",
       "0        '갈길 바쁜데' 총파업 발목잡은 삼성전자 노조..수주 리스크 우려   \n",
       "1             트립닷컴, 여름 휴가철 슈퍼세일…도쿄 왕복 항공권 5만원   \n",
       "2           공정위, 사행적 판매원 확장한 워너비데이터 시정명령…檢 고발   \n",
       "3           경기도, 리스로 고가 수입차 타는 고액 체납자 456명 적발   \n",
       "4          ‘엔비디아 훈풍’ 단일종목 채권혼합 ETF, 순자산 3배 급증   \n",
       "...                                       ...   \n",
       "5331          DL이앤씨, 6년 연속 건설업계 최고 신용등급 'AA-'   \n",
       "5332                   빙그레 더위사냥·생귤탱귤 '제로' 나왔다   \n",
       "5333  ‘골든타임을 지키는 다양한 가족’ 배유미 씨, 한미연 영상 공모전 대상   \n",
       "5334   조달청, 올해 1차 품질보증조달물품 지정서 수여…34개사 81개 물품   \n",
       "5335          맥도날드, 시각 장애인 위한 ‘음성 안내’ 키오스크 도입   \n",
       "\n",
       "                                                content  \n",
       "0     지난 5월29일 삼성전자 서초사옥 앞에서 전국삼성전자노동조합 파업 선언 기자회견이 ...  \n",
       "1     트립닷컴이 본격적인 여름 휴가철을 맞아 인기 여행지를 위한 할인 프로모션을 시작한다...  \n",
       "2     방문판매법 위반행위 제재…영업정지명령\\n\\n\\n\\n공정거래위원회 ⓒ연합뉴스[세종=데...  \n",
       "3     리스보증금 55억원 압류…가택수색·동산압류·형사고발도 추진(수원=연합뉴스) 김경태 ...  \n",
       "4     6개월새 순자산 5100억으로 쑥급등株에 채권혼합 안전성 더해퇴직연금서 '대규모 자...  \n",
       "...                                                 ...  \n",
       "5331  \"수익성 중심 내실 경영과 리스크 관리로 기업가치 제고\"DL이앤씨가 한국신용평가, ...  \n",
       "5332  [사진=빙그레 제공]빙그레가 당류가 빠진 빙과 신제품 2종을 선보인다고 오늘(2일)...  \n",
       "5333  한반도미래인구연구원,‘2024 다양한 가족의 재발견 영상 공모전’ 총 9팀 선정“입...  \n",
       "5334  안전운전 돕는 '교통안전표지'·'정수처리용 산기장치' 등… 납품검사 면제 등 혜택 ...  \n",
       "5335  지난해 9월 첫 설치…전국 직영 매장 100% 도입 완료\\n\\n\\n\\n한국맥도날드가...  \n",
       "\n",
       "[5336 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"/home/hadoop/project/third_project/data/20240702.csv\",encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
